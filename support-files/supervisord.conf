[unix_http_server]
file = {{.app_container_path}}run/supervisord.sock

[supervisorctl]
configuration = {{.app_container_path}}conf/supervisord.conf
serverurl = unix://{{.app_container_path}}run/supervisord.sock

[supervisord]
pidfile = {{.app_container_path}}run/supervisord.pid
logfile = {{.app_container_path}}logs/{{.app_code}}/supervisord.log
directory = {{.app_container_path}}

[rpcinterface:supervisor]
supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface

[program: {{.app_code}}_uwsgi]
command = /cache/.bk/env/bin/uwsgi --ini {{.app_container_path}}conf/{{.app_code}}.ini --enable-threads --single-interpreter
stdout_logfile = {{.app_container_path}}logs/{{.app_code}}/uwsgi.log
redirect_stderr = true
autorestart = true
environment = {{.environment}}

[program: {{.app_code}}_celery]
command = /cache/.bk/env/bin/python {{.app_container_path}}code/manage.py celery worker -n {{.node_name}}_{{.app_code}} -Q default,task_prepare_api -c 8 -l INFO --maxtasksperchild=50
directory = {{.app_container_path}}code/
stdout_logfile = {{.app_container_path}}logs/{{.app_code}}/celery.log
redirect_stderr = true
stopwaitsecs = 10
autorestart = true
environment = {{.environment}}

[program: {{.app_code}}_celery_pipeline]
command = /cache/.bk/env/bin/python {{.app_container_path}}code/manage.py celery worker -P threads -Q pipeline,pipeline_priority,periodic_task_queue_pipeline_priority,api_task_queue_pipeline_priority,service_schedule,service_schedule_priority,periodic_task_queue_service_schedule_priority,api_task_queue_service_schedule_priority -n {{.node_name}}_{{.app_code}}_pipeline_1 -l INFO -c 100 --maxtasksperchild=50
directory = {{.app_container_path}}code/
stdout_logfile = {{.app_container_path}}logs/{{.app_code}}/celery.log
redirect_stderr = true
stopwaitsecs = 10
autorestart = true
environment = {{.environment}}

[program: {{.app_code}}_celery_addtional_task]
command = /cache/.bk/env/bin/python {{.app_container_path}}code/manage.py celery worker -Q pipeline_additional_task,pipeline_additional_task_priority,node_auto_retry,timeout_node_execute,timeout_nodes_record -n {{.node_name}}_{{.app_code}}_addtional_task -l INFO -c 8 --maxtasksperchild=50
directory = {{.app_container_path}}code/
stdout_logfile = {{.app_container_path}}logs/{{.app_code}}/celery.log
redirect_stderr = true
stopwaitsecs = 10
autorestart = true
environment = {{.environment}}

[program: {{.app_code}}_celery_pipeline_statistic]
command = /cache/.bk/env/bin/celery worker -A blueapps.core.celery -P threads -Q pipeline_statistics_priority -n {{.node_name}}_{{.app_code}}_pipeline_statistic -l INFO -c 300
directory = {{.app_container_path}}code/
stdout_logfile = {{.app_container_path}}logs/{{.app_code}}/celery.log
redirect_stderr = true
stopwaitsecs = 10
autorestart = true
environment = {{.environment}}

[program: {{.app_code}}_celery_er_execute]
command = /cache/.bk/env/bin/celery worker -A blueapps.core.celery -P threads -Q er_execute,er_execute_api,er_execute_periodic_task -n {{.node_name}}_{{.app_code}}_er_execute -l INFO -c 300
directory = {{.app_container_path}}code/
stdout_logfile = {{.app_container_path}}logs/{{.app_code}}/celery.log
redirect_stderr = true
stopwaitsecs = 10
autorestart = true
environment = {{.environment}}

[program: {{.app_code}}_celery_er_schedule]
command = /cache/.bk/env/bin/celery worker -A blueapps.core.celery -P threads -Q er_schedule,er_schedule_api,er_schedule_periodic_task -n {{.node_name}}_{{.app_code}}_er_schedule -l INFO -c 300
directory = {{.app_container_path}}code/
stdout_logfile = {{.app_container_path}}logs/{{.app_code}}/celery.log
redirect_stderr = true
stopwaitsecs = 10
autorestart = true
environment = {{.environment}}

[program: {{.app_code}}_timeout_node_process]
command = /cache/.bk/env/bin/python manage.py node_timeout_process
directory = {{.app_container_path}}code/
stdout_logfile = {{.app_container_path}}logs/{{.app_code}}/{{.app_code}}.log
redirect_stderr = true
stopwaitsecs = 10
autorestart = true
environment = {{.environment}}
