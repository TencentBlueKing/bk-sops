# -*- coding: utf-8 -*-
"""
Tencent is pleased to support the open source community by making è“é²¸æ™ºäº‘PaaSå¹³å°ç¤¾åŒºç‰ˆ (BlueKing PaaS Community
Edition) available.
Copyright (C) 2017-2020 THL A29 Limited, a Tencent company. All rights reserved.
Licensed under the MIT License (the "License"); you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://opensource.org/licenses/MIT
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
"""
import logging
import time

from blueapps.contrib.celery_tools.periodic import periodic_task
from celery.schedules import crontab
from django.conf import settings
from django.db import transaction
from django.db.models import Q
from django.utils import timezone
from pipeline.contrib.periodic_task.models import PeriodicTaskHistory
from pipeline.contrib.statistics.models import ComponentExecuteData
from pipeline.models import PipelineInstance

from gcloud.analysis_statistics.models import TaskflowExecutedNodeStatistics, TaskflowStatistics
from gcloud.contrib.cleaner.models import ArchivedTaskInstance
from gcloud.contrib.cleaner.pipeline.bamboo_engine_tasks import get_clean_pipeline_instance_data
from gcloud.contrib.cleaner.signals import pre_delete_pipeline_instance_data
from gcloud.core.models import ProjectConfig
from gcloud.taskflow3.models import TaskFlowInstance
from gcloud.utils.decorators import time_record

logger = logging.getLogger("root")


def delete_records(queryset, field_name):
    """
    å¸¦é‡è¯•æœºåˆ¶çš„åˆ é™¤æ“ä½œï¼Œå¤„ç†Lock wait timeout
    """
    try:
        deleted_count = queryset.delete()[0]
        logger.info(f"[clean_expired_v2_task_data] Deleted {deleted_count} records from {field_name}")
        return deleted_count
    except Exception as e:
        error_msg = str(e)
        logger.exception(f"[clean_expired_v2_task_data] Failed to delete {field_name}: {error_msg}")
    return 0


def filter_clean_task_instances():
    """
    è¿‡æ»¤éœ€è¦æ¸…ç†çš„ä»»åŠ¡å®ä¾‹
    ä¼˜åŒ–åçš„ç‰ˆæœ¬ï¼šä½¿ç”¨å•æ¬¡æŸ¥è¯¢è·å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ä»»åŠ¡ï¼Œé¿å… N+1 æŸ¥è¯¢é—®é¢˜
    """
    validity_day = settings.V2_TASK_VALIDITY_DAY
    expire_time = timezone.now() - timezone.timedelta(days=validity_day)
    batch_num = settings.CLEAN_EXPIRED_V2_TASK_BATCH_NUM

    # è·å–æ‰€æœ‰é¡¹ç›®çš„ä»»åŠ¡æ¸…ç†é…ç½®
    project_clean_configs = {}
    for config in ProjectConfig.objects.filter(task_clean_configs__isnull=False).values(
        "project_id", "task_clean_configs"
    ):
        if config["task_clean_configs"] and isinstance(config["task_clean_configs"], dict):
            create_methods = config["task_clean_configs"].get("create_methods", [])
            if create_methods:  # åªä¿å­˜æœ‰æ•ˆçš„é…ç½®
                project_clean_configs[config["project_id"]] = create_methods

    if project_clean_configs:
        logger.info(
            f"[clean_expired_v2_task_data] Found {len(project_clean_configs)} "
            f"projects with custom clean configs: {list(project_clean_configs.keys())}"
        )

    # æ„å»ºåŸºç¡€æŸ¥è¯¢æ¡ä»¶
    base_q = Q(
        pipeline_instance__create_time__lt=expire_time,
        engine_ver=2,
        pipeline_instance__is_expired=False,
    )

    # å¦‚æœé…ç½®äº†å…¨å±€é¡¹ç›®èŒƒå›´ï¼Œæ·»åŠ é¡¹ç›®è¿‡æ»¤
    target_projects = settings.CLEAN_EXPIRED_V2_TASK_PROJECTS
    if target_projects:
        base_q &= Q(project_id__in=target_projects)

    # æ„å»ºæŸ¥è¯¢æ¡ä»¶ï¼šä½¿ç”¨ Q å¯¹è±¡ç»„åˆæ‰€æœ‰é¡¹ç›®çš„æ¡ä»¶ï¼Œå®ç°å•æ¬¡æŸ¥è¯¢
    query_conditions = Q()

    # æ·»åŠ æœ‰ç‰¹æ®Šé…ç½®çš„é¡¹ç›®æ¡ä»¶
    for project_id, create_methods in project_clean_configs.items():
        # è·³è¿‡ä¸åœ¨ç›®æ ‡é¡¹ç›®èŒƒå›´å†…çš„é¡¹ç›®
        if target_projects and project_id not in target_projects:
            continue

        # ä¸ºæ¯ä¸ªé¡¹ç›®åˆ›å»ºç‹¬ç«‹çš„æŸ¥è¯¢æ¡ä»¶
        project_q = Q(project_id=project_id, create_method__in=create_methods)
        query_conditions |= project_q

    # æ·»åŠ ä½¿ç”¨é»˜è®¤é…ç½®çš„é¡¹ç›®æ¡ä»¶
    configured_project_ids = list(project_clean_configs.keys())
    if configured_project_ids:
        # æ’é™¤å·²æœ‰ç‰¹æ®Šé…ç½®çš„é¡¹ç›®ï¼Œä½¿ç”¨å…¨å±€é…ç½®
        default_q = Q(create_method__in=settings.CLEAN_EXPIRED_V2_TASK_CREATE_METHODS) & ~Q(
            project_id__in=configured_project_ids
        )
    else:
        # æ‰€æœ‰é¡¹ç›®éƒ½ä½¿ç”¨å…¨å±€é…ç½®
        default_q = Q(create_method__in=settings.CLEAN_EXPIRED_V2_TASK_CREATE_METHODS)

    query_conditions |= default_q

    # æ‰§è¡Œå•æ¬¡æŸ¥è¯¢ï¼ŒæŒ‰åˆ›å»ºæ—¶é—´æ’åºç¡®ä¿å…¬å¹³æ€§
    # ä½¿ç”¨ id æ’åºè€Œä¸æ˜¯åˆ›å»ºæ—¶é—´ï¼Œå› ä¸º id æœ‰ç´¢å¼•ï¼Œæ€§èƒ½æ›´å¥½
    qs = (
        TaskFlowInstance.objects.filter(base_q & query_conditions)
        .order_by("id")
        .values("id", "pipeline_instance__instance_id", "project_id")[:batch_num]
    )

    ids = list(qs)

    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
    if ids:
        # ç»Ÿè®¡æ¯ä¸ªé¡¹ç›®æ¸…ç†çš„ä»»åŠ¡æ•°
        project_stats = {}
        for item in ids:
            project_id = item["project_id"]
            project_stats[project_id] = project_stats.get(project_id, 0) + 1

        logger.info(f"[clean_expired_v2_task_data] Total {len(ids)} tasks to clean from {len(project_stats)} projects")
        for project_id, count in sorted(project_stats.items()):
            config_info = (
                f"custom config (create_methods={project_clean_configs[project_id]})"
                if project_id in project_clean_configs
                else f"default config (create_methods={settings.CLEAN_EXPIRED_V2_TASK_CREATE_METHODS})"
            )
            logger.info(f"[clean_expired_v2_task_data] Project {project_id}: {count} tasks ({config_info})")

    return ids


@periodic_task(run_every=(crontab(*settings.CLEAN_EXPIRED_V2_TASK_CRON)), ignore_result=True, queue="task_data_clean")
@time_record(logger)
def clean_expired_v2_task_data():
    """
    æ¸…é™¤è¿‡æœŸçš„ä»»åŠ¡æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬

    ä¼˜åŒ–ç‚¹ï¼š
    1. æŒ‰ä¾èµ–å…³ç³»é¡ºåºåˆ é™¤ï¼Œé¿å…å¤–é”®å†²çª
    2. åˆ†å°æ‰¹æ¬¡å¤„ç†ï¼Œå‡å°‘å•æ¬¡é”å®šçš„æ•°æ®é‡
    3. æ·»åŠ é‡è¯•æœºåˆ¶ï¼Œå¤„ç†ä¸´æ—¶é”å†²çª
    """
    if not settings.ENABLE_CLEAN_EXPIRED_V2_TASK:
        logger.info("Skip clean expired task data")
        return

    logger.info("Start clean expired task data (optimized)...")

    try:
        ids = filter_clean_task_instances()
        if not ids:
            logger.info("[clean_expired_v2_task_data] No tasks to clean")
            return

        task_ids = [item["id"] for item in ids]
        pipeline_instance_ids = [item["pipeline_instance__instance_id"] for item in ids]

        logger.info(
            f"[clean_expired_v2_task_data] Total {len(task_ids)} tasks to clean, "
            f"task_ids: {task_ids[:10]}{'...' if len(task_ids) > 10 else ''}"
        )

        # ğŸ”§ ä¼˜åŒ–1: åˆ†å°æ‰¹æ¬¡å¤„ç†ä»»åŠ¡ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†å¤ªå¤š
        task_batch_size = 20

        for i in range(0, len(pipeline_instance_ids), task_batch_size):
            batch_pipeline_ids = pipeline_instance_ids[i : i + task_batch_size]
            batch_task_ids = task_ids[i : i + task_batch_size]

            logger.info(
                f"[clean_expired_v2_task_data] Processing batch {i//task_batch_size + 1}/"
                f"{(len(task_ids) + task_batch_size - 1)//task_batch_size}, "
                f"task_ids: {batch_task_ids}"
            )

            try:
                with transaction.atomic():
                    _clean_task_batch(batch_pipeline_ids, batch_task_ids)
            except Exception as e:
                logger.exception(f"[clean_expired_v2_task_data] Error cleaning batch {batch_task_ids}: {e}")
                # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹ï¼Œä¸è¦å› ä¸ºä¸€æ‰¹å¤±è´¥è€Œåœæ­¢æ‰€æœ‰æ¸…ç†
                continue

            # æ‰¹æ¬¡é—´çŸ­æš‚ä¼‘æ¯ï¼Œé‡Šæ”¾æ•°æ®åº“å‹åŠ›
            time.sleep(0.5)

        logger.info("[clean_expired_v2_task_data] All batches processed")
    except Exception as e:
        logger.exception(f"[clean_expired_v2_task_data] error: {e}")


def _clean_task_batch(pipeline_instance_ids, task_ids):
    """
    æ¸…ç†ä¸€æ‰¹ä»»åŠ¡æ•°æ®

    æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. æŒ‰æ­£ç¡®é¡ºåºåˆ é™¤è¡¨ï¼Œé¿å…å¤–é”®å†²çªå’Œæ­»é”
    2. æ·»åŠ é‡è¯•æœºåˆ¶å¤„ç†ä¸´æ—¶é”å†²çª
    """
    data_to_clean = get_clean_pipeline_instance_data(pipeline_instance_ids)
    tasks = TaskFlowInstance.objects.filter(id__in=task_ids)
    data_to_clean.update({"tasks": tasks})

    # å‘é€é¢„åˆ é™¤ä¿¡å·
    pre_delete_pipeline_instance_data.send(sender=TaskFlowInstance, data=data_to_clean)

    instance_fields = ["tasks", "pipeline_instances"]

    # ğŸ”§ ä¼˜åŒ–2: å®šä¹‰åˆ é™¤é¡ºåºï¼ŒæŒ‰ä¾èµ–å…³ç³»ä»å¶å­åˆ°æ ¹
    # å…ˆåˆ é™¤ä¾èµ–è¡¨ï¼ˆå­è¡¨ï¼‰ï¼Œå†åˆ é™¤ä¸»è¡¨ï¼Œé¿å…å¤–é”®å†²çªå’Œæ­»é”
    delete_order = [
        # 1. èŠ‚ç‚¹ç›¸å…³çš„è¯¦ç»†æ•°æ®ï¼ˆæœ€åº•å±‚ï¼‰
        "callback_data",
        "schedules_list",
        "execution_history_list",
        "execution_data_list",
        "state_list",
        "data_list",
        # 2. èŠ‚ç‚¹é…ç½®å’Œç­–ç•¥
        "retry_node_list",
        "timeout_node_list",
        "node_list",
        # 3. ä¸Šä¸‹æ–‡å’Œè¿›ç¨‹æ•°æ®
        "context_value",
        "context_outputs",
        "process",
        # 4. å‘¨æœŸä»»åŠ¡å’Œä¸šåŠ¡å±‚æ•°æ®
        "periodic_task_history",
        "nodes_in_pipeline",
        # 5. å¿«ç…§å’Œæ ‘ä¿¡æ¯
        "execution_snapshot",
        "tree_info",
        # 6. æœ€åå¤„ç†å®ä¾‹å’Œä»»åŠ¡
        "tasks",
        "pipeline_instances",
    ]

    for field_name in delete_order:
        if field_name not in data_to_clean:
            continue

        qs_or_list = data_to_clean[field_name]

        # å¤„ç†åˆ—è¡¨ç±»å‹ï¼ˆåˆ†æ‰¹çš„QuerySetï¼‰
        if field_name.endswith("_list") and isinstance(qs_or_list, list):
            logger.info(f"[clean_expired_v2_task_data] clean field: {field_name}, " f"{len(qs_or_list)} batch data")

            # ğŸ”§ ä¿®å¤: ä½¿ç”¨å¾ªç¯è€Œä¸æ˜¯åˆ—è¡¨æ¨å¯¼å¼ï¼Œé¿å…å†…å­˜é—®é¢˜
            for idx, qs in enumerate(qs_or_list):
                delete_records(qs, f"{field_name}[{idx}]")

        # å¤„ç†pipeline_instances - åªæ ‡è®°è¿‡æœŸï¼Œä¸åˆ é™¤
        elif field_name == "pipeline_instances":
            updated_count = qs_or_list.update(is_expired=True)
            logger.info(f"[clean_expired_v2_task_data] Updated {updated_count} pipeline_instances")

        # å¤„ç†éœ€è¦åˆ é™¤çš„è¡¨
        elif field_name not in instance_fields or settings.CLEAN_EXPIRED_V2_TASK_INSTANCE:
            logger.info(f"[clean_expired_v2_task_data] clean field: {field_name}")
            delete_records(qs_or_list, field_name)

    logger.info(f"[clean_expired_v2_task_data] Successfully cleaned batch: {task_ids}")


@periodic_task(run_every=(crontab(*settings.ARCHIVE_EXPIRED_V2_TASK_CRON)), ignore_result=True, queue="task_data_clean")
@time_record(logger)
def archive_expired_v2_task_data():
    """
    å½’æ¡£è¿‡æœŸä»»åŠ¡æ•°æ®
    """
    if not settings.ENABLE_ARCHIVE_EXPIRED_V2_TASK:
        logger.info("Skip archive expired v2 task data")
        return

    logger.info("Start archive expired task data...")
    try:
        validity_day = settings.V2_TASK_VALIDITY_DAY
        expire_time = timezone.now() - timezone.timedelta(days=validity_day)

        batch_num = settings.ARCHIVE_EXPIRED_V2_TASK_BATCH_NUM

        tasks = (
            TaskFlowInstance.objects.select_related("pipeline_instance")
            .filter(pipeline_instance__create_time__lt=expire_time, engine_ver=2, pipeline_instance__is_expired=1)
            .order_by("id")[:batch_num]
        )
        if not tasks:
            logger.info("No expired task data to archive")
            return
        archived_task_objs = [
            ArchivedTaskInstance(
                task_id=task.id,
                project_id=task.project_id,
                name=task.pipeline_instance.name,
                template_id=task.pipeline_instance.template_id,
                task_template_id=task.template_id,
                template_source=task.template_source,
                create_method=task.create_method,
                create_info=task.create_info,
                creator=task.pipeline_instance.creator,
                create_time=task.pipeline_instance.create_time,
                executor=task.pipeline_instance.executor,
                recorded_executor_proxy=task.recorded_executor_proxy,
                start_time=task.pipeline_instance.start_time,
                finish_time=task.pipeline_instance.finish_time,
                is_started=task.pipeline_instance.is_started,
                is_finished=task.pipeline_instance.is_finished,
                is_revoked=task.pipeline_instance.is_revoked,
                engine_ver=task.engine_ver,
                is_child_taskflow=task.is_child_taskflow,
                snapshot_id=task.pipeline_instance.snapshot_id,
                current_flow=task.current_flow,
                is_deleted=task.is_deleted,
                extra_info=task.extra_info,
            )
            for task in tasks
        ]
        task_ids = [item.id for item in tasks]
        pipeline_ids = [item.pipeline_instance_id for item in tasks]
        pipeline_instance_ids = [item.pipeline_instance.instance_id for item in tasks]
        with transaction.atomic():
            # PeriodicTaskHistory å¯¹ PipelineInstance æœ‰ do nothing å¤–é”®ï¼Œéœ€è¦ä¿è¯å¼•ç”¨å…³ç³»è¢«æ¸…é™¤ï¼Œé¿å…å¤–é”®çº¦æŸå¯¼è‡´æ¸…ç†å¤±è´¥
            # è¿‡æœŸä»»åŠ¡çš„ periodic_task_history å¿…å®šå¯æ¸…ç†
            periodic_task_history_qs = PeriodicTaskHistory.objects.filter(
                pipeline_instance_id__in=pipeline_instance_ids
            )
            if periodic_task_history_qs:
                logger.info(
                    f"[archive_expired_v2_task_data] delete periodic task nums: {len(periodic_task_history_qs)}, "
                    f"e.x.: {periodic_task_history_qs.values_list('pk', flat=True)[:3]}..."
                )
                periodic_task_history_qs.delete()
            archived_task_instances = ArchivedTaskInstance.objects.bulk_create(archived_task_objs)
            logger.info(f"[archive_expired_v2_task_data] archived nums: {len(archived_task_instances)}")
            TaskFlowInstance.objects.filter(id__in=task_ids).delete()
            logger.info(f"[archive_expired_v2_task_data] delete task nums: {len(task_ids)}, e.x.: {task_ids[:3]}...")
            PipelineInstance.objects.filter(id__in=pipeline_ids).delete()
            logger.info(
                f"[archive_expired_v2_task_data] delete pipeline nums: {len(pipeline_ids)}, "
                f"e.x.: {pipeline_ids[:3]}..."
            )
        logger.info(f"[archive_expired_v2_task_data] success archive tasks: {task_ids[:3]}...")
    except Exception as e:
        logger.exception(f"[archive_expired_v2_task_data] error: {e}")


@periodic_task(
    run_every=(crontab(*settings.CLEAN_EXPIRED_STATISTICS_CRON)), ignore_result=True, queue="task_data_clean"
)
@time_record(logger)
def clear_statistics_info():
    """
    æ¸…é™¤è¿‡æœŸçš„ç»Ÿè®¡ä¿¡æ¯
    """
    if not settings.ENABLE_CLEAN_EXPIRED_STATISTICS:
        logger.info("Skip clean expired statistics data")
        return

    logger.info("Start clean expired statistics data...")
    try:
        validity_day = settings.STATISTICS_VALIDITY_DAY
        expire_time = timezone.now() - timezone.timedelta(days=validity_day)
        batch_num = settings.CLEAN_EXPIRED_STATISTICS_BATCH_NUM

        data_to_clean = [
            {"model": TaskflowStatistics, "time_field": "create_time", "order_field": "create_time"},
            {
                "model": TaskflowExecutedNodeStatistics,
                "time_field": "instance_create_time",
                "order_field": "instance_create_time",
            },
            {"model": ComponentExecuteData, "time_field": "archived_time", "order_field": "id"},
        ]
        for data in data_to_clean:
            model = data["model"]
            time_field = data["time_field"]
            order_field = data["order_field"]
            qs = model.objects.filter(**{f"{time_field}__lt": expire_time}).order_by(order_field)[:batch_num]
            ids_to_delete = list(qs.values_list("id", flat=True))
            if ids_to_delete:
                model.objects.filter(id__in=ids_to_delete).delete()
                logger.info(
                    f"[clear_statistics_info] clean model {model.__name__} deleted nums: {len(ids_to_delete)}, "
                    f"e.x.: {ids_to_delete[:3]}..."
                )
        logger.info("[clear_statistics_info] success clean statistics")
    except Exception as e:
        logger.exception(f"Failed to clear expired statistics data: {e}")
